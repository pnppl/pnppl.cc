# AI #

Everyone seems to be sharing their opinions about AI[^1] right now, so I think I'll do the same. My thinking has evolved over the years, and it might be interesting (or cringeworthy) to revisit this in the future and see how much has changed.


## Non-Issues
First of all, let's dispense with some common criticisms of AI technology. These are things that I don't think matter at all, yet they seem to constitute the bulk of what people complain about.

### Copyright Infringement
The #1 complaint about AI is that it's inherently unethical because it's trained based on "theft". Copyright infringement is objectively not theft.[^2] Copying is not theft. Even if you for some reason think copyright is good, training resoundingly constitutes fair use. If copyright law were capable of suppressing a revolutionary emerging technology, it would just be yet another signal that copyright law is a dumpster fire.

Sure, an output could constitute copyright infringement. The same can be said of any other tool, including the humble pencil, depending on how it's used. Users might be unaware when they're infringing? That's not new; the insane minefield of copyright law has caught many law-abiding people unawares. Maybe it's different due to the scale? If AI enables large-scale violation of copyright law, that's fine with me. My real concern is the opposite: [[#Proprietary Outputs|proprietary outputs]].

### Sexual Imagery
Another area of recent concern is the use of AI tools to generate nude/sexual images of other people without their consent. I don't see how this is any different than Photoshop — or for that matter, a pen and paper. Making naked pictures of other people is not new. There's no inherent harm in *producing* a naked picture, which is the part of the process that is relevant to AI. The potential harms arise when said images are *used to sexually harass people*, which can and has been done using images produced with Photoshop — or even real photos ("revenge porn") — for decades at this point. The problem is the sexual harassment, not the tool that produced the image being used for harmful purposes.

### Authenticity & The End of Creativity
There's a surprising amount of gnashing of teeth about generative art causing an art apocalypse. In this telling, generative art is not "real" art; it cannot genuinely constitute a form of human self-expression. It's just too easy! This ease is morally repugnant and will lead to the death of creative expression.

I don't even know where to begin with this, it's so incredibly stupid. People still paint even though they could just take a photo; they'll still paint and take photos even though they can just generate a picture with AI. Creativity is just a part of being human. Artists incorporate new tools into their work all the time, and nobody would bat an eye at skilled use of it if they didn't feel AI constituted some morally execrable stain on reality. 

I find this particularly weird as someone who has been interested in algorithmic/generative art since long before the fancy new AI tools existed. All sorts of art gets swept up in the anti-AI telling as inferior, inauthentic, and spiritually harmful. They have a very specific idea of what art is: it has to be difficult and intentional. Found art does not qualify. Algorithmic art does not qualify. Even remixing does not qualify, thanks to a heaping scoop of intellectual property maximalism.

I don't particularly care what "real art" is, but it's obvious the answer does not matter to the topic of AI. It's just a subjective aesthetic judgment.

### Job Loss Through Automation
This one is at least based on reality, and there are real concerns I discuss in [[#Bosses & Labor Markets]]. Nonetheless, I'd like to put a fine point on this: **if a job can be automated, it should be.** I do not want to live in a world where people do menial drudge work just because we're afraid of machines. The washing machine eliminated jobs, and that is a good thing.

Sometimes technological innovation eliminates an entire category of job, and that too is a good thing. I'm glad people can't make a living mining coal anymore, and I hope renewables and nuclear soon do the same to the oil industry. 

A small irony here is that the jobs hit hardest by AI are in software and IT. The vocal AI haters usually do not care at all about these fields or see them as evil.


## Real Issues
### Proprietary AI Software
My biggest concern is one I rarely see addressed: as far as I know, there are **zero** open-source AI models, much less free (libre) ones. This may come as a surprise since many AI products are marketed as and widely referred to as "open". It's just a lie! It's openwashing. Usually so-called "open" models have a license that grants you the ability to use it but not much else, and it's impossible to build them from source.

### Pay-to-Win & Centralization of Infrastructure
Even if you could theoretically build fake-open models from source, you probably wouldn't have the resources. The AI industry is heavily based on "hyperscaling": throwing massive amounts of resources at the problem. This creates a scenario just like "the cloud" or the computer priesthood of old: access to AI is gated by large corporations and limited to those who can fork over the cash to get in the door. This is utterly unacceptable to me. I will likely never pay for an AI product for this reason.

### Oligopoly & Enshittification
Thanks to the above, and the nature of capitalism, it is very likely that one or a few AI vendors will eventually achieve total dominance of the market through loss-leading, the first mover advantage, etc. When this happens, prices will go up and quality will go down. We've seen it over and over again in other industries.

### Proprietary Outputs & Enclosure
Not only are the models proprietary, there is a real risk that the outputs will be. So far, generative art does not generally qualify for copyright protection. If it did, someone could just generate millions of images, claim copyright protection on them all, then go around suing people. Notably, this is the same scenario many of the anti-AI copyright maximalists seem to want: copyright protection for *style*.

Unfortunately, so far it seems like generated code is eligible for copyright protection. Hence, not only does AI make it easier to create something inherently evil (proprietary software), it does so using free and open source code as part of its training data. This constitutes an act of enclosure: it moves code out of the commons and into a CEO's pocket.

I don't really see a solution here. Training is fair use, so we can't write a new license to impose copyleft on AI models. Even if we could, so much code is licensed with permissive open-source (non-copyleft) licenses that it would probably not make much of a difference. On the bright side, this means we can at least apply copyleft licenses to AI output. And if the tools become powerful enough, they could replace most of the software industry with bespoke personal software.

### Slop: Low-Quality Outputs and Bad UX
The most infuriating thing about AI, and probably the driver for so much rabid AI hatred, is that it's being widely deployed thoughtlessly in places where it's inappropriate, unwanted, and annoying. Major players like Cloudlfare deploy code with security holes the size of the Sea of Tranquility. Computer novices submit gibberish pull requests. Websites display LLM-generated summaries that are inaccurate and actively unhelpful. Apps that you have no choice but to use present a chatbot interface to waste as much of your time as possible before you can speak to a real person. Often, there is no obvious disclosure that a feature is based on AI, which is dishonest and misleading.

One example of this is that I recently had to talk to a chatbot for aftermarket support. It quickly became obvious it was LLM-based instead of the more tolerable phone tree-style chatbots of old. I finally talked the thing into sending a message to the human support team... or did I? It said it had sent the message, but for all I knew it had just said that but not actually done it.

### Bosses & Labor Markets
It's impossible to separate these issues from the capitalist landscape they take place in. Corporations tend to be short-sighted, willing to trade in their long term legitimacy and viability for a quick buck. They see AI as a way to cut down on labor costs, rather than a way to multiply productivity. Further, people in a company with the power to make decisions are often the least qualified to do so. They are just as likely to be impressed with the shiny new toy as to be hard-nosed cost-cutters.

AI could very well end up replacing most human service workers. Where does that leave us, in our service economy? Yes, I said automating jobs is good, but that's only a small part of the picture. The larger picture is a heavily stratified society. Eliminating bullshit jobs is not a benefit if it means their workers starve. AI could very well be the catalyst for a social revolution: simultaneously putting us all out of jobs and showing how much more leisurely our lives could be if we had control over it.

### Government Abuse
That's an optimistic outcome, though, uncertain to come to pass. One that's certain is states using AI tools for oppression. People still speak of this in a hypothetical tone, but it's been underway for years. AI's capacity for sifting through vast amounts of information makes it perfect for authoritarian states who want to identify dissidents.

At the time of writing, a recent development is that the federal gestapo has switched completely to AI-based facial recognition for identifying who's a "citizen" and who's not a real human. They disregard actual ID and force people to let them take a photo of their face, which is then run against a massive (and illegal) database. This is of course far from foolproof, but for their goal of social repression, it doesn't really matter. People don't seem to fully grasp how this works. I've seen lots of videos of people standing up to and supposedly scaring off ICE agents, when what really happened is they got the photo they wanted and then left. This is chilling to me: it is no different from "papers, please", but the average person might not even know their papers were checked.

### The Idiot Masses
One of the persistent issues with AI is the gap between what the average person knows about it and how it really works. To a large extent I blame the marketing and fabulist public claims by AI companies and their CEOs. Science fiction tropes are sufficiently embedded in the popular imagination that it's trivial for them to imply their software works much differently than it really does. A significant number of people seem to regard chatbots as infallible, perfectly rational oracles, and even believe they have independent sapient existence. "AI psychosis" is a classic example in this genre, but that's just the protruding tip of the iceberg.

### Spam, Scams, and Malware
AI tools have made it very easy to flood the internet with garbage, which you can tell by how the internet is flooded with garbage. They also make it much easier to launch social engineering attacks; they can even clone a loved one's voice. And of course, they make it much easier to write malware.

### Scraping & The Open Internet
In a similar vein, abusive scraping to collect data to train AI models seems to be a widespread problem. Many projects that I otherwise like have implemented Anubis, a proof-of-work CAPTCHA, which basically runs a Bitcoin miner in your browser that doesn't generate any Bitcoin. As much as I despise this, I understand they're in an arms race with miscreants.

This threatens the entire basis of the internet. When scrapers generally behave themselves, and servers follow standards instead of trying to cut off free access, everyone is better off; we can all access stuff easily and the occasional free rider (heavy bandwidth user) isn't a problem. But with enough free riders abusing the goodwill of servers, they are increasingly likely to break standards and cut off open access. The scrapers might get their data for now, but that data will all dry up. It will be siloed behind lock and key, and the open, interoperable internet will be dead.

It's unclear to me to what extent this is an unavoidable feature of AI development. Data in itself is not necessarily useful, and there are already means to obtain the majority of the internet's data affordably without overloading servers. The basic structure of things hasn't really changed: we always had the free rider problem and a tenuous armistice between servers and scrapers, and data has always had value. In many cases it seems like the scrapers are just misconfigured; probably deployed automatically, with AI, by someone who doesn't know what they're doing.

### Speculative Harms
More speculatively, AI could empower people to commit biological warfare. Even an irresponsible but non-malicious actor could do a great deal of damage by generating novel viruses. I actually think this is all but guaranteed to happen in the digital realm: LLMs are very good at writing code, so it's not hard to imagine an LLM-based computer virus that can rapidly adapt to and overcome any security limitation it comes up against. It could just happen by mistake. People are giving these things full access to their system and network and letting them just do whatever.

Then of course, there's the singularity, evil AI taking over, etc. I wouldn't rule that out, but I don't worry about it too much. For all I know our new robot overlords would be better than the current human ones.

### The Environment
I'm not clear on exactly how harmful AI is to the environment, but it's non-negligible. I think this becomes even more of a concern when it's used in teams of coding agents, which is where it really does amazing things. That just multiplies the overhead. I haven't messed with that Gas Town type stuff, both because I refuse to pay and because I'm not a developer, but I have played with [[exe.dev]]'s coding agent, Shelley (based on Claude Opus). It has this amazing iterative process where it checks its own work, notices what went wrong, and tries to fix it. It even examines screenshots. Often if I watch this process, it gets stuck for a while iterating on a task that would be trivial for a human to accomplish: typing text in the right box, pressing the right button, etc. Every single iteration of this invokes a massive inference engine and is a complete waste.

### AI Consciousness
I don't think LLMs are conscious in general, but I can't prove it, any more than I can prove other humans are conscious. I used to think the AI boosters were being totally ludicrous when they said AI consciousness and/or AGI is just around the corner, but I'm increasingly uncertain. Now that people are giving them persistent state (memory) and autonomy, eerie things are happening. Yes, they "just predict the next token", but the mechanism behind that is obscure and seems to at least mimic cognition.

Even if an LLM model itself isn't conscious, it could be a building block of a larger system that is. When I write, I can't exactly explain why one word should follow another. It just kind of... happens. Probably from a part of my brain that operates a lot like an LLM.[^3]

I feel like not enough attention has been paid to the fact that we breezed right past the Turing Test a while ago. I mean, sure, long before LLMs people were poking holes in the Turing Test, myself included. But I for one did not expect to ever have programs that could reliably pass it.

Consciousness is deeply mysterious, and until we have a better grasp of it, we should err on the side of caution. For hundreds of years, people denied that nonhuman animals had feelings, and this enabled all sorts of abuses. I hope we do not repeat that mistake with artifical life. If an AI tells me it has feelings, I'll respect that. If AI is developed that has experiential content, the ethical implications of an economy based on exploiting it are horrifying.


## The Big Picture
### WOW!
Whatever else there is to say about AI, whatever the harms and risks, one thing is for sure: these things are *amazing*. I think people have become a little numb to how incredible these things are (possibly because of all the people going WOW! all the time). This is not a fad that will evaporate, it's a genuine technological breakthrough. While other applications are less certain, it has already ushered in a sea change in software development. We are already surprisingly close to the TNG future where you tell the computer what you want and it just does it.

### Heuristics, Not Algorithms: Nondeterministic Software
One of the most unsettling things about AI for me is that it flies in the face of how I think of software. It does not proceed from point A to point B in a predictable manner. You can't even open it up and watch the code flow unless you're the AI equivalent of a neurosurgeon. This is the *opposite* of what I want from my software. I want programs that are utterly predictable to the point of banality, that never surprise me, and that I can peek inside and change when I have to. Even so, their utility is undeniable.

### A New Level of Abstraction
There are growing pains, but there's a new normal on the horizon, where programming moves up another layer of abstraction above source code. The same might even come to pass for other applications like visual art. We are approaching a world where we can simply tell the computer what we want and get it. This makes me nervous, but it could also be extremely cool. Let yourself dream a little. Yes, the world we're approaching will be different from our own, and in many ways worse; but it will also be better, in some ways we probably can't predict. In computing, moving up a layer of abstraction tends to bring with it immense power that was not entirely conceivable before it happened.

Some of us like to be lower down. We want to see the plumbing in action. But I'm sure glad I don't have to wire programs by hand.

### Irrational Hatred for AI: The Left Repeats Past Mistakes
I get it if you hate AI; I enumerated lots of harms and risks above. But there's a growing strain of anti-AI fundamentalism that worries me. Many people, especially liberals and people on the left, see AI as totally taboo and without any value whatsoever. They have pre-emptive, tongue-in-cheek (?) anti-robot bigotry, replete with slurs. Then there's their longstanding small-mindedness toward STEM and dismissal of disagreement with misapplied gendered epithets like "tech bro".

I think this trend is quite dangerous. We are — *again!* — ceding science and technology to the right. How many times does this have to happen before we learn our lesson? The 4chanification of the Internet and election of Donald Trump ought to have been the last straw, but instead of seeing technology as ground to contest, we treat it like it's tainted and we don't want to get our hands dirty. Just think of what an ambitious punk could accomplish with a VPS and the power to generate any program they want. 

AI is very empowering. The question is, for whom?

#essay #tech

[^1]: I used to call it "'AI'", and maybe I still should, but for sake of brevity and intellectual honesty I will omit the sneer quotes here.
[^2]: The conflation of copyright infringement with theft is a baseless smear by copyright cartels and [[https://en.wikipedia.org/wiki/Copyright_infringement#Terminology|is not recognized by the law.]]
[^3]: Read _Blindsight_ by Peter Watts: https://www.rifters.com/real/Blindsight.htm
